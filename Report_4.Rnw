\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{url}
\usepackage{float}
\usepackage{fullpage}
\setkeys{Gin}{width=0.50\textwidth}

\titleformat{\section}
  {\normalfont\scshape}{\thesection}{1em}{}

\title{Report 3}
\author{Vanessa Machuca and Luis Espino}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle


<<echo=FALSE,results=hide>>=
library(skimr)
library(ggplot2)
library(dplyr)
library(broom)
library(mosaic)
library(readr)
library(lattice)
library(investr)
library(robustbase)
require(gridExtra)
require(ISLR)
require(glmnet)
require(splines)
@

<<echo=FALSE,results=hide>>=
student_por <- read_delim("~/Desktop/MATH158_SLMs/math158_machuspino/student-por.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
student_por_n <- student_por %>% 
  mutate(studytime_num = ifelse(studytime == 1, 1, 
         ifelse(studytime == 2, 3.5,
         ifelse(studytime == 3, 7.5, 12))))
@


\section{Task 1 - Sparse and Smooth Linear Models}

Introduction 

The data we are working with looks at student achievement in Portuguese course at two secondary education Portuguese schools. We found the data through the UCI Machine Learning Repository. The observational units for the dataset are students. The dataset includes demographic variables like student’s school name (binary), sex (binary), age (numeric), and whether the student lives in an urban or rural area. 

It also includes variables that relate to family, including family size, parents cohabitation status (living apart or together), mother and father's education (ordinal), mother and father’s job ('teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at$\_$home' or 'other'), student’s guardian (mother, father or other), and quality of family relations ($1$ being very bad to $5$ being excellent). 

School-related variables include home to school travel time (ordinal), weekly study time (ordinal), and number of past class failures (1-3 if $n<3$, 4 otherwise). For each of two courses, students were given a graded after the first and second period as well as a final grade.  
Academic routine, habits and performance were also measured. These variables included travel time from home to school (nominal), weekly study time (nominal), number of past class failures (nominal), reason to choose school (categorical), and number of absences (numeric). Additionally, educational resources were assessed through binary measurements of extra educational support, family educational support, extra paid classes within the course subject, extracurricular activities, attended nursery school, college aspiration, and home internet access.

Finally, nominal variables that measured more personal aspect of life included frequency of outings with friends, workday alcohol consumption, weekend alcohol consumption, current health status, and being in a romantic relationship (binary). 

RR and LASSO

We will now run ridge regression (RR) and LASSO on the full varaible set, using cross validation to find lambda, and compare them. First up, RR. 

<<echo=FALSE>>=
set.seed(47)
lambda.grid =10^seq(5,-2, length =100)
#remove the response variable and cat. vars. from the data set
new_student <- student_por_n[-c(1,2,4:6,9:12,16:23,33)]
pr.ridge.cv <-cv.glmnet(as.matrix(new_student), student_por_n$G3,alpha=0, lambda = lambda.grid, standardize=TRUE)
pr.ridge.cv$lambda.min
@

We omit categorical variables, as RR does not seem to like them very much. The lambda value which yields the lowest SSE is $0.0225702$. Onward to LASSO. 

<<echo=FALSE>>=
set.seed(47)
pr.lasso.cv <- cv.glmnet(as.matrix(new_student), student_por_n$G3,
alpha=1, lambda = lambda.grid, standardize=TRUE)
pr.lasso.cv$lambda.min
@

The lambda value which yields the lowest SSE is $0.03125716$. Now to compare these two models by looking at their coefficients. Previously, we looked at plots of cefficients.

MLR 

<<>>=
fwdselec <- lm(G3 ~ failures + school + sex + higher + health + Mjob, data=student_por_n)
summary(fwdselec)
@

Pairs plot comparing LASSO and RR.
<<>>=
coefols <- as.numeric(coef(fwdselec)[c(-1)])
coef.ridge <- as.numeric(coef(pr.ridge.cv, s="lambda.min")[c(-1)])
coef.lasso <- as.numeric(coef(pr.lasso.cv, s="lambda.min")[c(-1)])
compare <- cbind(coefols,coef.ridge,coef.lasso)
pairs(compare)
@

The pairs plot shows that the linear model coefficients don’t have high correlation
with either the ridge regression or lasso coefficients. Most nonzero constants for OLS are very close to zero for ridge regression, and the biggest ridge regression coefficients are not the biggest OLS coefficients. The coefficient plot between lasso and OLS is fairly similar to the coefficient plot between ridge regression and OLS, except the coefficients hug around the line x=0 tighter. Between ridge regression and lasso, most of the coefficients that hug around 0 (on both sides of 0) for ridge are positive and even closer to 0 for lasso. For both plots, only three coefficients seem to be visibly larger relative to all the other ones.

<<echo=FALSE>>=
pr.ridge <-glmnet(as.matrix(new_student), student_por_n$G3, alpha=0,lambda = 0.0225702, standardize=TRUE)
pr.lasso <-glmnet(as.matrix(new_student), student_por_n$G3, alpha=1,lambda = 0.03125716, standardize=TRUE)
@

Below is a plot comparing responses predicted using MLR, RR, and LASSO. 
<<echo=FALSE>>=
##compare models 
fwd.pred <- predict(fwdselec, newdata = student_por_n)
ridge.pred <- predict(pr.ridge, newx = as.matrix(new_student),
s = "lambda.min")
lasso.pred <- predict(pr.lasso, newx = as.matrix(new_student),
s = "lambda.min")

sum((fwd.pred - student_por_n$G3)^2)
sum((ridge.pred - student_por_n$G3)^2)
sum((lasso.pred - student_por_n$G3)^2)

ggplot(new_student, aes(x = student_por_n$G3)) + geom_point(aes(y = ridge.pred, colour = "ridge.pred"), fill = "blue", shape=1) + geom_point(aes(y = fwd.pred, colour = "fwd.pred"), fill="orange", alpha = 1/3, shape=1) + geom_point(aes(y = lasso.pred, colour = "lasso.pred"), fill = "green", alpha = 1/5, shape=1) + ylab(label = "Predicted Values") + xlab(label = "Observed Values") 
@

Now to choose one variable and run smoothing spline and kernel smoother models on it. We change the parameters so that we have four different models for each method. We chose "absences", as this was found to be the most significant variable in report 1.  Let us first use regression splines for four different degrees of freedom: 3,4,5, and 6

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
dislims <- range(student_por_n$absences)
disgrid=seq(from=dislims[1],to=dislims[2])

for(i in 3:4){
rs <- lm(G3 ~ bs(absences,df=i,degree=3),data=student_por_n)

rs.pred <- predict(rs,newdata = list(absences=disgrid), se=TRUE)
rs.se <- cbind(rs.pred$fit +2* rs.pred$se.fit,rs.pred$fit -2*rs.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(i," = df: SSE=", round(sum(rs$resid^2),3),sep=""),outer =F)
lines(disgrid, rs.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, rs.se ,lwd =1, col =" blue",lty =3)
}
@

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
dislims <- range(student_por_n$absences)
disgrid=seq(from=dislims[1],to=dislims[2])

for(i in 5:6){
rs <- lm(G3 ~ bs(absences,df=i,degree=3),data=student_por_n)

rs.pred <- predict(rs,newdata = list(absences=disgrid), se=TRUE)
rs.se <- cbind(rs.pred$fit +2* rs.pred$se.fit,rs.pred$fit -2*rs.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(i," = df: SSE=", round(sum(rs$resid^2),3),sep=""),outer =F)
lines(disgrid, rs.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, rs.se ,lwd =1, col =" blue",lty =3)
}
@

And now onward to LOESS.

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
spanvals = c(.6,.75,1,2)

for(i in 1:2){
lr <- loess(G3 ~ absences,span=spanvals[i],data=student_por_n)

lr.pred <- predict(lr,data.frame(absences=disgrid), se=TRUE)
lr.se <- cbind(lr.pred$fit +2* lr.pred$se.fit,lr.pred$fit -2*lr.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(spanvals[i]," = span: SSE=", round(sum(lr$resid^2),3),sep=""),outer =F)
lines(disgrid, lr.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, lr.se ,lwd =1, col =" blue",lty =3)
}
@

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
spanvals = c(.6,.75,1,2)

for(i in 3:4){
lr <- loess(G3 ~ absences,span=spanvals[i],data=student_por_n)

lr.pred <- predict(lr,data.frame(absences=disgrid), se=TRUE)
lr.se <- cbind(lr.pred$fit +2* lr.pred$se.fit,lr.pred$fit -2*lr.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(spanvals[i]," = span: SSE=", round(sum(lr$resid^2),3),sep=""),outer =F)
lines(disgrid, lr.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, lr.se ,lwd =1, col =" blue",lty =3)
}
@

Without cross validating, we would choose the best model. Here's why.
Now to summarize our results for this section. In it, we will comment on anything of interest that occurred. Did anything surprise us? 

\section{Task 2 - Something New}



\section{Task 3 - Summary}



\end{document}