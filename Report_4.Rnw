\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{url}
\usepackage{float}
\usepackage{fullpage}
\setkeys{Gin}{width=0.50\textwidth}

\titleformat{\section}
  {\normalfont\scshape}{\thesection}{1em}{}

\title{Report 3}
\author{Vanessa Machuca and Luis Espino}

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle


<<echo=FALSE,results=hide>>=
library(skimr)
library(ggplot2)
library(dplyr)
library(broom)
library(mosaic)
library(readr)
library(lattice)
library(investr)
library(robustbase)
require(gridExtra)
require(ISLR)
require(glmnet)
require(splines)
require(pls)
require(caret)
require(chemCal)
@

<<echo=FALSE,results=hide>>=
student_por <- read_delim("~/Desktop/MATH158_SLMs/math158_machuspino/student-por.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
student_por_n <- student_por %>% 
  mutate(studytime_num = ifelse(studytime == 1, 1, 
         ifelse(studytime == 2, 3.5,
         ifelse(studytime == 3, 7.5, 12))))
@


\section{Task 1 - Sparse and Smooth Linear Models}

Introduction 

The data we are working with looks at student achievement in Portuguese course at two secondary education Portuguese schools. We found the data through the UCI Machine Learning Repository. The observational units for the dataset are students. The dataset includes demographic variables like student’s school name (binary), sex (binary), age (numeric), and whether the student lives in an urban or rural area. 

It also includes variables that relate to family, including family size, parents cohabitation status (living apart or together), mother and father's education (ordinal), mother and father’s job ('teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at$\_$home' or 'other'), student’s guardian (mother, father or other), and quality of family relations ($1$ being very bad to $5$ being excellent). 

School-related variables include home to school travel time (ordinal), weekly study time (ordinal), and number of past class failures (1-3 if $n<3$, 4 otherwise). For each of two courses, students were given a graded after the first and second period as well as a final grade.  
Academic routine, habits and performance were also measured. These variables included travel time from home to school (nominal), weekly study time (nominal), number of past class failures (nominal), reason to choose school (categorical), and number of absences (numeric). Additionally, educational resources were assessed through binary measurements of extra educational support, family educational support, extra paid classes within the course subject, extracurricular activities, attended nursery school, college aspiration, and home internet access.

Finally, nominal variables that measured more personal aspect of life included frequency of outings with friends, workday alcohol consumption, weekend alcohol consumption, current health status, and being in a romantic relationship (binary). 

RR and LASSO

We will now run ridge regression (RR) and LASSO on the full varaible set, using cross validation to find lambda, and compare them. First up, RR. 

<<echo=FALSE>>=
set.seed(47)
lambda.grid =10^seq(5,-2, length =100)
#remove the response variable and cat. vars. from the data set
new_student <- student_por_n[-c(1,2,4:6,9:12,16:23,33)]
pr.ridge.cv <-cv.glmnet(as.matrix(new_student), student_por_n$G3,alpha=0, lambda = lambda.grid, standardize=TRUE)
pr.ridge.cv$lambda.min
@

We omit categorical variables, as RR does not seem to like them very much. The lambda value which yields the lowest SSE is $0.0225702$. Onward to LASSO. 

<<echo=FALSE>>=
set.seed(47)
pr.lasso.cv <- cv.glmnet(as.matrix(new_student), student_por_n$G3,
alpha=1, lambda = lambda.grid, standardize=TRUE)
pr.lasso.cv$lambda.min
@

The lambda value which yields the lowest SSE is $0.03125716$. Now to compare these two models by looking at their coefficients. Previously, we looked at plots of cefficients.

MLR 

<<>>=
fwdselec <- lm(G3 ~ failures + school + sex + higher + health + Mjob, data=student_por_n)
summary(fwdselec)
@

Pairs plot comparing LASSO and RR.
<<>>=
coefols <- as.numeric(coef(fwdselec)[c(-1)])
coef.ridge <- as.numeric(coef(pr.ridge.cv, s="lambda.min")[c(-1)])
coef.lasso <- as.numeric(coef(pr.lasso.cv, s="lambda.min")[c(-1)])
compare <- cbind(coefols,coef.ridge,coef.lasso)
pairs(compare)
@

The pairs plot shows that the linear model coefficients don’t have high correlation
with either the ridge regression or lasso coefficients. Most nonzero constants for OLS are very close to zero for ridge regression, and the biggest ridge regression coefficients are not the biggest OLS coefficients. The coefficient plot between lasso and OLS is fairly similar to the coefficient plot between ridge regression and OLS, except the coefficients hug around the line x=0 tighter. Between ridge regression and lasso, most of the coefficients that hug around 0 (on both sides of 0) for ridge are positive and even closer to 0 for lasso. For both plots, only three coefficients seem to be visibly larger relative to all the other ones.

<<echo=FALSE>>=
pr.ridge <-glmnet(as.matrix(new_student), student_por_n$G3, alpha=0,lambda = 0.0225702, standardize=TRUE)
pr.lasso <-glmnet(as.matrix(new_student), student_por_n$G3, alpha=1,lambda = 0.03125716, standardize=TRUE)
@

Below is a plot comparing responses predicted using MLR, RR, and LASSO. 
<<echo=FALSE>>=
##compare models 
fwd.pred <- predict(fwdselec, newdata = student_por_n)
ridge.pred <- predict(pr.ridge, newx = as.matrix(new_student),
s = "lambda.min")
lasso.pred <- predict(pr.lasso, newx = as.matrix(new_student),
s = "lambda.min")

sum((fwd.pred - student_por_n$G3)^2)
sum((ridge.pred - student_por_n$G3)^2)
sum((lasso.pred - student_por_n$G3)^2)

ggplot(new_student, aes(x = student_por_n$G3)) + geom_point(aes(y = ridge.pred, colour = "ridge.pred"), fill = "blue", shape=1) + geom_point(aes(y = fwd.pred, colour = "fwd.pred"), fill="orange", alpha = 1/3, shape=1) + geom_point(aes(y = lasso.pred, colour = "lasso.pred"), fill = "green", alpha = 1/5, shape=1) + ylab(label = "Predicted Values") + xlab(label = "Observed Values") 
@

Now to choose one variable and run smoothing spline and kernel smoother models on it. We change the parameters so that we have four different models for each method. We chose "absences", as this was found to be the most significant variable in report 1.  Let us first use regression splines for four different degrees of freedom: 3,4,5, and 6

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
dislims <- range(student_por_n$absences)
disgrid=seq(from=dislims[1],to=dislims[2])

for(i in 3:4){
rs <- lm(G3 ~ bs(absences,df=i,degree=3),data=student_por_n)

rs.pred <- predict(rs,newdata = list(absences=disgrid), se=TRUE)
rs.se <- cbind(rs.pred$fit +2* rs.pred$se.fit,rs.pred$fit -2*rs.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(i," = df: SSE=", round(sum(rs$resid^2),3),sep=""),outer =F)
lines(disgrid, rs.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, rs.se ,lwd =1, col =" blue",lty =3)
}
@

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
dislims <- range(student_por_n$absences)
disgrid=seq(from=dislims[1],to=dislims[2])

for(i in 5:6){
rs <- lm(G3 ~ bs(absences,df=i,degree=3),data=student_por_n)

rs.pred <- predict(rs,newdata = list(absences=disgrid), se=TRUE)
rs.se <- cbind(rs.pred$fit +2* rs.pred$se.fit,rs.pred$fit -2*rs.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(i," = df: SSE=", round(sum(rs$resid^2),3),sep=""),outer =F)
lines(disgrid, rs.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, rs.se ,lwd =1, col =" blue",lty =3)
}
@

And now onward to LOESS.

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
spanvals = c(.6,.75,1,2)

for(i in 1:2){
lr <- loess(G3 ~ absences,span=spanvals[i],data=student_por_n)

lr.pred <- predict(lr,data.frame(absences=disgrid), se=TRUE)
lr.se <- cbind(lr.pred$fit +2* lr.pred$se.fit,lr.pred$fit -2*lr.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(spanvals[i]," = span: SSE=", round(sum(lr$resid^2),3),sep=""),outer =F)
lines(disgrid, lr.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, lr.se ,lwd =1, col =" blue",lty =3)
}
@

<<fig=TRUE, height=3, echo=FALSE>>=
par(mfrow=c(1,2))
spanvals = c(.6,.75,1,2)

for(i in 3:4){
lr <- loess(G3 ~ absences,span=spanvals[i],data=student_por_n)

lr.pred <- predict(lr,data.frame(absences=disgrid), se=TRUE)
lr.se <- cbind(lr.pred$fit +2* lr.pred$se.fit,lr.pred$fit -2*lr.pred$se.fit)

plot(student_por_n$absences, student_por_n$G3, xlim=dislims ,cex =.5, pch=19,col =" darkgrey ", xlab="Absences", ylab="Grade") 
title(paste(spanvals[i]," = span: SSE=", round(sum(lr$resid^2),3),sep=""),outer =F)
lines(disgrid, lr.pred$fit ,lwd =2, col =" blue") 
matlines(disgrid, lr.se ,lwd =1, col =" blue",lty =3)
}
@

Without cross validating, we would choose the best model. Here's why.
Now to summarize our results for this section. In it, we will comment on anything of interest that occurred. Did anything surprise us? 

\section{Task 2 - Something New}

\subsection{Principal Components Regression}

Next, we perform pricipal components regression on our data. Principal components regression is a dimension-reduction technique for regression. Instead of regressing on $p$ predictors, we regress on $M$ principal components, where principal components are linear combinations of the predictors, and ideally $M<<p$. The first part of PCR is principle component analysis, where, through an iterative process, the principle components are found. This process is as follows: 

\begin{enumerate}
\item Principal component 1 is computed by finding the direction in which the observations vary the most. This is equivalent to finding the line whose distance is shortest to all the observations. 
\item Principal component 2 is computed by finding the line that is orthogonal to principal component 1 and that points in the direction in which the observations on this orthogonal plane now vary the most. 
\item Iterate as in step (2) until you've found $p$ principal components. 
\end{enumerate}

The second part of PCR is to perform a linear regression on the principal components that capture most of your variability. OLS obtains estimates for $\beta_j$ in $$y_i = \beta_0 + \sum \beta_jx_{ij} + \epsilon_i$$ that minimize $$SSE = \sum_{i=1}^{n}\left(Y_i - b_0 - \sum_{j=1}^{p-1}b_jX_{ij}\right)^2$$. On the other hand, PCR obtains estimates for $\beta_j^\prime$ in $$y_i = \beta_0^\prime + \sum_{j=1}^{p\prime}\beta_j^\prime \alpha_{ij} + \epsilon_i$$ that minimize $$SSE = \sum_{i=1}^{n}\left(Y_i - b_0^\prime - \sum_{j=1}^{M-1}b_j^\prime \alpha_{ij}\right)^2$$.

The number of principal components is typically chosen through CV and standardize the predictors before doing PCA ensures that all predictors have equal weight in obtaining the principal components. Otherwise, predictors with higher variance will have a greater impact on the final model.

The idea is that through $M$ principal components, where ideally $M<p$, you can capture most of the variability in the data and the relationship with the response in less dimensions than you would through your $p$ predictors. In this way, PCR also may help prevent overfitting. A key underlying assumption of PCR is that the direction in which the $p$ predictors show the most variation are the directions associated with $Y$. 

PCR is particularly useful for data in which the response can be accurately modeled using only few principal components. We chose PCR because based on our previous models, it seems that most of the variation in our data could potentially be captured in a few components and thus, PCR could give us a more parsimonious model compared to OLS. We also chose it because it could help address any multicolinearity present in our data.

PCR solves issues of multicolinearity, and dimensionality, where the number of observations is less than the number of predictors. 

<<results=hide,fig=TRUE, height=3, echo=FALSE>>=
set.seed (47)
pcr_model <- pcr(G3~., data = student_por_n, scale = TRUE, validation = "CV")
summary(pcr_model)
# Plot the cross validation MSE
validationplot(pcr_model, val.type="MSEP")
@


<<echo=FALSE>>=
set.seed(47)
inTrain <- createDataPartition(y=student_por_n$G3, p=.747, list=FALSE)
pcr.train <- student_por_n[inTrain,]
pcr.test <- student_por_n[-inTrain,]
pcr.train <- pcr.train[,-c(31:32)]
y.test <- pcr.test[,33]
pcr.test <- pcr.test[,-c(31:33)]


pcr_model <- pcr(G3~., data = pcr.train, scale = TRUE, validation = "CV")
pcr_pred <- predict(pcr_model, pcr.test, ncomp = 7)
mean((pcr_pred - y.test)^2)
@


We chose to have 7 principal components in our model based on the CV error. The test error for this this number of principal components is 6.92.

\subsection{Inverse Predictions}

Regression models of Y on X can also be used to make an inverse prediction of X on Y. That is, predicting what X might have resulted in the observed Y value. In the context of our data set, we were interested in making inverse predictions of the variable $studytime_num$ - weekly hours studies - based on a student's grades. This might be useful for recommending study time to achieve certain grades. Obtaining inverse predictions is a matter of a simple reformulation of our usual estimated regression model

$$\hat{Y}=b_{0} + b_{1}X$$

to get 

$$\hat{X}_{h(new)} = \frac{Y_{h(new)}-b_{0}}{b_{1}}$$

<<>>=
ggplot(student_por_n,aes(studytime_num,G3))+geom_point()
@

<<>>=
ip.lm <- lm(G3 ~ studytime_num, data=student_por_n)
summary(ip.lm)
@

This gives us the following model

$$\hat{Y}=10.92+0.264X$$

and so 

$$\hat{X}=3.787-41.34Y$$

Now to make our inverse predictions:

<<>>=
inverse.predict(ip.lm,15)
@

The predicted study time per week for a student with a grade of 15 is 15.46 hours per week. 

Some statisticians argue that it makes more sense to just regress X on Y. We will now do this for $study_time$ and $G3$ and see how these results might differ from what we got above.

<<>>=
ggplot(student_por_n,aes(G3,studytime_num))+geom_point()
ir.lm <- lm(studytime_num ~ G3, data=student_por_n)
summary(ir.lm)
@

$$\hat{X}=1.244 + 0.209Y$$



\subsection{Added Variable Plots}



\section{Task 3 - Summary}



\end{document}