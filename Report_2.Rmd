---
title: "Report 2"
author: "Vanessa Machuca and Luis Espino "
date: "2/12/2018"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(skimr)
library(ggplot2)
library(dplyr)
library(broom)
library(mosaic)
library(readr)
library(lattice)
library(investr)
library(robustbase)
require(gridExtra)
```

Our hypothesis is that a linear relationship exists between number of absences (absences) and final grade (G3). More specifically, weâ€™d predict that a negative relationship exists between the two.  

### UNTRANSFORMED VARIABLES ###

absences vs. final grade plot using ggplot

```{r, eval=FALSE}
ggplot(student_por, aes(absences, G3)) + geom_point() + geom_smooth(method='lm') 
```

residual vs. predicted using xyplot
```{r, eval=FALSE}
G3_lm <- lm(G3 ~ absences, data=student_por)
xyplot(rstandard(G3_lm) ~ fitted(G3_lm), pch=18)
```
There seems to be a negative linear relatinoshios between number of absences and final grade. It is slight, though. 


99% CI for slope parameter $\beta_1$

```{r, eval=FALSE}
tidy(G3_lm, conf.int = TRUE, conf.level = 0.99)
```
The CI is (-0.134, 0.0067). It contains zero, so we cannot be confident that the population slope is not zero. That is, there may not be a linear relationship between number of absences and final grade. Let's try transforming the variables, and then removing some outliers.

### REMOVING OUTLIERS ###

Let's remove the final grades for student who were absent 30 or more times - 2 outliers in total.

```{r, eval=FALSE}
student_porfiltered<- filter(student_por, absences < 30)
ggplot(student_porfiltered, aes(absences, G3)) + geom_point() + geom_smooth(method='lm') 
filteredG3_lm <- lm(G3 ~ absences, data=student_por_28)
tidy(filteredG3_lm, conf.int = TRUE, conf.level = 0.99)
```
The 99% CI for the population slope is now (-0.1591,-0.0104). This interval does not contain 0, so we can be confident that the population slope is not zero - there is a linear relationship between number of absences and final grade. Because we looked at a subset of the explanatory variables, though, the model is only appropriate for fewer than 30 absences.

### ASSESS FIT OF MODEL W/O OUTLIERS ###

The data seem to be fairly symmetric about the model line, but lacks constanct variability. We will now compute a prediction interval for an individual response at an interesting x value.  Let's look at x=15.

```{r, eval=FALSE}
newdata = data.frame(absences=15)
predict(filteredG3_lm, newdata, interval="predict")
```

For a student with 15 absences, we get a predicted final grade of 10.9 and prediction interval of (4.58,17.3). This is a very wide interval, as can be expected both of a prediction interval (relative to a CI) and of higher x-value. Now to find the $R^2$. 

```{r, eval=FALSE}
summary(filteredG3_lm)$r.squared
```

We get an $R^2$ value of 0.013. This tells us that number of absences explains 1.3% of variability in final grades - quite a small percentage. 
We expected a stronger linear relationship between number of absences and final grade. That said, it's safe to say that number absences is likely related to other variables in the data set. In the future, then, we'd like to explore colinearity between absences and other variables like travel time, number of past class failures, and quality of family relationships.  We are interesting in exploring the relationship between parent education and student school performance. The former may indicate whether or not a student who grew up in a family environment that promotes education, which might affect that student's performance. 

### Simultaneous Inference

```{r, eval=FALSE}
working.hotelling.bonferroni.intervals <- function(x, y) {
  y <- as.matrix(y)
  x <- as.matrix(x)
  n <- length(y)

  # Get the fitted values of the linear model
  fit <- lm(y ~ x)
  fit <- fit$fitted.values
  
  # Find standard error as defined above
  se <- sqrt(sum((y - fit)^2) / (n - 2)) * 
    sqrt(1 / n + (x - mean(x))^2 / 
           sum((x - mean(x))^2))

  # Calculate B and W statistics for both procedures.
  W <- sqrt(2 * qf(p = 0.95, df1 = 2, df2 = n - 2))
  B <- 1-qt(.95/(2 * 3), n - 2)
  N <- 1-qt(.95/2, n - 2)

  # Compute the simultaneous confidence intervals
  
  # Working-Hotelling
  wh.upper <- fit + W * se
  wh.lower <- fit - W * se
  
  # Bonferroni
  bon.upper <- fit + B * se
  bon.lower <- fit - B * se
  
  # unadjusted
  non.upper <- fit + N * se
  non.lower <- fit - N * se
  
  xy <- data.frame(cbind(x,y))
  
  # Plot the Working-Hotelling intervals
  wh <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=wh.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, wh.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Working-Hotelling')
  
  # Plot the Bonferroni intervals
  bonn <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=bon.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, bon.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Bonferroni')
  
  # Plot the unadjusted intervals
  non <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=non.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, non.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Unadjusted')
  
  
  grid.arrange(non, wh, bonn, ncol = 3)
  
  # Collect results of procedures into a data.frame and return
  res <- data.frame(round(cbind(N, W, B), 3), row.names = c('Result'))
  colnames(res) <- c('N', 'W', 'B')
  
  return(res)
}

working.hotelling.bonferroni.intervals(student_porfiltered$absences, student_porfiltered$G3)
```


```{r, eval=FALSE, echo=FALSE, fig.keep=TRUE}
working.hotelling.bonferroni.pintervals <- function(x, y) {
  y <- as.matrix(y)
  x <- as.matrix(x)
  n <- length(y)

  # Get the fitted values of the linear model
  fit <- lm(y ~ x)
  fit <- fit$fitted.values
  
  # Find standard error as defined above
  se <- sqrt(sum((y - fit)^2) / (n - 2)) * 
    sqrt(1+(1 / n) + (x - mean(x))^2 / 
           sum((x - mean(x))^2))

  # Calculate B and W statistics for both procedures.
  W <- sqrt(2 * qf(p = 0.95, df1 = 2, df2 = n - 2))
  B <- 1-qt(.95/(2 * 3), n - 2)
  N <- 1-qt(.95/2, n - 2)

  # Compute the simultaneous confidence intervals
  
  # Working-Hotelling
  wh.upper <- fit + W * se
  wh.lower <- fit - W * se
  
  # Bonferroni
  bon.upper <- fit + B * se
  bon.lower <- fit - B * se
  
  # unadjusted
  non.upper <- fit + N * se
  non.lower <- fit - N * se
  
  xy <- data.frame(cbind(x,y))
  
  # Plot the Working-Hotelling intervals
  wh <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=wh.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, wh.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Working-Hotelling')
  
  # Plot the Bonferroni intervals
  bonn <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=bon.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, bon.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Bonferroni')
  
  # Plot the unadjusted intervals
  non <- ggplot(xy, aes(x=x, y=y)) + 
    geom_point(size=2.5) + 
    geom_line(aes(y=fit, x=x), size=1) + 
    geom_line(aes(x=x, y=non.upper), colour='blue', linetype='dashed', size=1) + 
    geom_line(aes(x=x, non.lower), colour='blue', linetype='dashed', size=1) +
    labs(title='Unadjusted')
  
  grid.arrange(non, wh, bonn, ncol = 3)
  
  # Collect results of procedures into a data.frame and return
  res <- data.frame(round(cbind(N, W, B), 3), row.names = c('Result'))
  colnames(res) <- c('N', 'W', 'B')
  
  return(res)
}

working.hotelling.bonferroni.pintervals(student_porfiltered$absences, student_porfiltered$G3)

```


The level of significance, $\alpha$, and power for a single test differs from the level of significance and power for a family of tests. For example, t-statistics based on the same sample data and MSE will be dependent on each other. Adjusting for multiple comparisons allows us to more accurately determine the actual level of significance and power for a family of tests and thus decreases the likelihood of type I errors. 

